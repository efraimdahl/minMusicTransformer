{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programms\\Anaconda\\envs\\mtmt\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import src.utils as utils\n",
    "import src.representation as representation\n",
    "import src.dataset as dataset\n",
    "import src.music_x_transformers as music_x_transformers\n",
    "import src.advUtils as advUtils\n",
    "from torch import nn\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load configurations\n",
    "train_args = utils.load_json(\".\\pre_trained_models\\mmt_sod_ape_training_logs.json\")\n",
    "encoding = representation.load_encoding(\"encoding.json\")\n",
    "\n",
    "sos = encoding[\"type_code_map\"][\"start-of-song\"]\n",
    "eos = encoding[\"type_code_map\"][\"end-of-song\"]\n",
    "beat_0 = encoding[\"beat_code_map\"][0]\n",
    "beat_4 = encoding[\"beat_code_map\"][4]\n",
    "beat_16 = encoding[\"beat_code_map\"][16]\n",
    "\n",
    "# Load training/testing/demo Data\n",
    "data_set = advUtils.convert_extract_load(train_args,encoding, json_dir = \"./data/test/json\",repr_dir=\"./data/test/repr\")\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    data_set,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    collate_fn=dataset.MusicDataset.collate,\n",
    ")\n",
    "\n",
    "test_loader = data_loader\n",
    "train_loader = data_loader\n",
    "valid_loader = data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Creating the model...\")\n",
    "model = music_x_transformers.MusicXTransformer(\n",
    "    dim=train_args[\"dim\"],\n",
    "    encoding=encoding,\n",
    "    depth=train_args[\"layers\"],\n",
    "    heads=train_args[\"heads\"],\n",
    "    max_seq_len=train_args[\"max_seq_len\"],\n",
    "    max_beat=train_args[\"max_beat\"],\n",
    "    rotary_pos_emb=train_args[\"rel_pos_emb\"],\n",
    "    use_abs_pos_emb=train_args[\"abs_pos_emb\"],\n",
    "    emb_dropout=train_args[\"dropout\"],\n",
    "    attn_dropout=train_args[\"dropout\"],\n",
    "    ff_dropout=train_args[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"../pre_trained_models/TranferLearned.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MusicXTransformer(\n",
       "  (decoder): MusicAutoregressiveWrapper(\n",
       "    (net): MusicTransformerWrapper(\n",
       "      (token_emb): ModuleList(\n",
       "        (0): TokenEmbedding(\n",
       "          (emb): Embedding(5, 512)\n",
       "        )\n",
       "        (1): TokenEmbedding(\n",
       "          (emb): Embedding(257, 512)\n",
       "        )\n",
       "        (2): TokenEmbedding(\n",
       "          (emb): Embedding(13, 512)\n",
       "        )\n",
       "        (3): TokenEmbedding(\n",
       "          (emb): Embedding(129, 512)\n",
       "        )\n",
       "        (4): TokenEmbedding(\n",
       "          (emb): Embedding(33, 512)\n",
       "        )\n",
       "        (5): TokenEmbedding(\n",
       "          (emb): Embedding(65, 512)\n",
       "        )\n",
       "      )\n",
       "      (pos_emb): AbsolutePositionalEmbedding(\n",
       "        (emb): Embedding(1024, 512)\n",
       "      )\n",
       "      (emb_dropout): Dropout(p=0.2, inplace=False)\n",
       "      (project_emb): Identity()\n",
       "      (attn_layers): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.2, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.2, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (4): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (5): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.2, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (6): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (7): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.2, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (8): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (9): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.2, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (10): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "              (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (11): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): None\n",
       "              (2): None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (1): GELU()\n",
       "                )\n",
       "                (1): Identity()\n",
       "                (2): Dropout(p=0.2, inplace=False)\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (to_logits): ModuleList(\n",
       "        (0): Linear(in_features=512, out_features=5, bias=True)\n",
       "        (1): Linear(in_features=512, out_features=257, bias=True)\n",
       "        (2): Linear(in_features=512, out_features=13, bias=True)\n",
       "        (3): Linear(in_features=512, out_features=129, bias=True)\n",
       "        (4): Linear(in_features=512, out_features=33, bias=True)\n",
       "        (5): Linear(in_features=512, out_features=65, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Adapter(nn.Module):\n",
    "    \"\"\"\n",
    "    The adapters first project the original\n",
    "    d-dimensional features into a smaller dimension, m, apply\n",
    "    a nonlinearity, then project back to d dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, size = 1, model_dim = 1):\n",
    "        super().__init__()\n",
    "        self.adapter_block = nn.Sequential(\n",
    "            nn.Linear(model_dim, size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(size, model_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ff_out = self.adapter_block(x)\n",
    "        # Skip connection\n",
    "        adapter_out = ff_out + x\n",
    "\n",
    "        return adapter_out\n",
    "    \n",
    "class Adaptered(nn.Module):\n",
    "    def __init__(self, orig_layer):\n",
    "        super().__init__()\n",
    "        self.orig_layer = orig_layer\n",
    "        self.adapter = Adapter()\n",
    "\n",
    "    def forward(self, *x):\n",
    "        orig_out = self.orig_layer(*x)\n",
    "        output = self.adapter.forward(orig_out[0].unsqueeze(0))[0]\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "smodel = music_x_transformers.MusicXTransformer(\n",
    "    dim=train_args[\"dim\"],\n",
    "    encoding=encoding,\n",
    "    depth=1,\n",
    "    heads=train_args[\"heads\"],\n",
    "    max_seq_len=50,\n",
    "    max_beat=train_args[\"max_beat\"],\n",
    "    rotary_pos_emb=train_args[\"rel_pos_emb\"],\n",
    "    use_abs_pos_emb=train_args[\"abs_pos_emb\"],\n",
    "    emb_dropout=train_args[\"dropout\"],\n",
    "    attn_dropout=train_args[\"dropout\"],\n",
    "    ff_dropout=train_args[\"dropout\"],\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('decoder.net.token_emb.0.emb.weight',\n",
       "              tensor([[-0.0439, -0.0196, -0.0295,  ..., -0.0066,  0.0447,  0.1069],\n",
       "                      [ 0.0420, -0.0388,  0.0729,  ...,  0.0420, -0.0550, -0.0572],\n",
       "                      [-0.0469, -0.0630, -0.0076,  ...,  0.0310,  0.0314,  0.0062],\n",
       "                      [ 0.0256,  0.0755,  0.0219,  ..., -0.0900,  0.0364, -0.1459],\n",
       "                      [ 0.1592,  0.1332,  0.0025,  ...,  0.0783,  0.1121, -0.0762]])),\n",
       "             ('decoder.net.token_emb.1.emb.weight',\n",
       "              tensor([[ 0.0095, -0.0110,  0.0819,  ...,  0.0710, -0.0171,  0.0040],\n",
       "                      [ 0.0809, -0.0916,  0.0653,  ...,  0.0990,  0.0136,  0.0673],\n",
       "                      [ 0.0736,  0.0889, -0.0930,  ..., -0.0009,  0.0398, -0.0549],\n",
       "                      ...,\n",
       "                      [ 0.0306,  0.0332, -0.0249,  ...,  0.0843,  0.0102, -0.0121],\n",
       "                      [-0.0288,  0.0407, -0.0359,  ..., -0.0993, -0.0673,  0.0151],\n",
       "                      [-0.0070, -0.0769,  0.0825,  ..., -0.0260,  0.0291, -0.0012]])),\n",
       "             ('decoder.net.token_emb.2.emb.weight',\n",
       "              tensor([[ 0.0402, -0.0014,  0.0088,  ...,  0.0492,  0.0080, -0.0153],\n",
       "                      [ 0.0021, -0.0131,  0.0009,  ..., -0.0511, -0.0190, -0.0133],\n",
       "                      [-0.0647,  0.0341, -0.0401,  ...,  0.0274, -0.0203,  0.0304],\n",
       "                      ...,\n",
       "                      [ 0.0510, -0.0093, -0.0778,  ...,  0.0809,  0.0677,  0.0476],\n",
       "                      [-0.0411, -0.0253, -0.0934,  ...,  0.0476, -0.0445, -0.1060],\n",
       "                      [-0.0498, -0.1563,  0.0801,  ..., -0.1658,  0.1476, -0.0133]])),\n",
       "             ('decoder.net.token_emb.3.emb.weight',\n",
       "              tensor([[ 0.1493,  0.0015, -0.0498,  ...,  0.0769,  0.0302, -0.0663],\n",
       "                      [ 0.0097,  0.1134,  0.0373,  ..., -0.0307,  0.0715,  0.0285],\n",
       "                      [-0.0398, -0.1025,  0.0086,  ..., -0.0312, -0.0318,  0.0734],\n",
       "                      ...,\n",
       "                      [-0.0525, -0.0037, -0.0652,  ...,  0.0739, -0.0384, -0.0308],\n",
       "                      [ 0.0833,  0.0744, -0.0223,  ...,  0.0538,  0.0119, -0.1857],\n",
       "                      [ 0.0247, -0.0316,  0.1016,  ...,  0.0057, -0.0677, -0.0427]])),\n",
       "             ('decoder.net.token_emb.4.emb.weight',\n",
       "              tensor([[ 0.0288,  0.0238, -0.0854,  ...,  0.0155, -0.0348,  0.0415],\n",
       "                      [ 0.0188,  0.0278, -0.0756,  ..., -0.0685,  0.0008,  0.0418],\n",
       "                      [-0.0333,  0.0146,  0.0751,  ..., -0.0778, -0.0840,  0.0664],\n",
       "                      ...,\n",
       "                      [-0.1312,  0.0806,  0.0092,  ...,  0.0509, -0.0608, -0.0694],\n",
       "                      [ 0.0785,  0.1059, -0.0580,  ...,  0.0319,  0.0094, -0.1095],\n",
       "                      [ 0.1051,  0.0966,  0.0082,  ...,  0.0022,  0.1390,  0.0417]])),\n",
       "             ('decoder.net.token_emb.5.emb.weight',\n",
       "              tensor([[ 1.0211e-01,  1.4589e-02,  9.8484e-02,  ..., -3.7331e-02,\n",
       "                       -1.0907e-01, -3.4137e-02],\n",
       "                      [ 3.6085e-02, -4.4307e-05,  3.9855e-02,  ..., -8.0771e-02,\n",
       "                        6.8064e-02,  2.4167e-02],\n",
       "                      [ 1.3047e-02,  2.3126e-02, -5.8101e-02,  ...,  6.0277e-02,\n",
       "                        3.2976e-03, -1.6946e-01],\n",
       "                      ...,\n",
       "                      [-4.0314e-02, -1.8675e-02,  3.4748e-02,  ...,  8.2007e-02,\n",
       "                       -5.8900e-03,  3.6099e-02],\n",
       "                      [ 7.4845e-02, -6.9530e-03, -2.9833e-02,  ...,  2.8959e-02,\n",
       "                        5.0840e-02, -4.4075e-02],\n",
       "                      [-7.2959e-02,  9.0018e-02,  7.7122e-02,  ..., -2.2163e-02,\n",
       "                       -9.7073e-02,  3.7082e-02]])),\n",
       "             ('decoder.net.pos_emb.emb.weight',\n",
       "              tensor([[ 1.3715,  0.1404,  1.7087,  ...,  0.2673, -0.2725, -0.1308],\n",
       "                      [-0.9669,  0.5270, -0.6173,  ..., -0.2302,  1.8692,  2.0514],\n",
       "                      [ 1.2523, -0.7082, -1.9994,  ...,  0.2532, -0.3460,  0.5320],\n",
       "                      ...,\n",
       "                      [ 0.5874, -0.5343, -1.3729,  ...,  0.1360,  0.5695,  1.1070],\n",
       "                      [ 0.7215,  0.8108,  0.3785,  ...,  0.2305, -0.5224, -0.6648],\n",
       "                      [-1.5912, -0.8339,  1.9106,  ..., -0.7904,  0.7628,  1.1839]])),\n",
       "             ('decoder.net.attn_layers.layers.0.0.0.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('decoder.net.attn_layers.layers.0.0.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('decoder.net.attn_layers.layers.0.1.to_q.weight',\n",
       "              tensor([[ 0.0063, -0.0401, -0.0407,  ...,  0.0418,  0.0039, -0.0136],\n",
       "                      [ 0.0290, -0.0025, -0.0023,  ..., -0.0401, -0.0121,  0.0243],\n",
       "                      [ 0.0166,  0.0233,  0.0166,  ...,  0.0252, -0.0085,  0.0266],\n",
       "                      ...,\n",
       "                      [-0.0038, -0.0035, -0.0080,  ...,  0.0278,  0.0107,  0.0381],\n",
       "                      [ 0.0142, -0.0328,  0.0177,  ...,  0.0264,  0.0254, -0.0400],\n",
       "                      [-0.0064, -0.0319, -0.0345,  ..., -0.0325,  0.0418, -0.0119]])),\n",
       "             ('decoder.net.attn_layers.layers.0.1.to_k.weight',\n",
       "              tensor([[ 0.0362, -0.0404,  0.0427,  ...,  0.0031,  0.0051,  0.0358],\n",
       "                      [-0.0440, -0.0056, -0.0312,  ..., -0.0318, -0.0317, -0.0039],\n",
       "                      [-0.0018,  0.0204, -0.0411,  ...,  0.0251,  0.0004,  0.0406],\n",
       "                      ...,\n",
       "                      [-0.0380,  0.0041,  0.0015,  ...,  0.0238,  0.0171, -0.0034],\n",
       "                      [ 0.0087,  0.0417,  0.0289,  ...,  0.0063, -0.0058,  0.0239],\n",
       "                      [ 0.0320,  0.0062, -0.0231,  ..., -0.0337,  0.0221,  0.0400]])),\n",
       "             ('decoder.net.attn_layers.layers.0.1.to_v.weight',\n",
       "              tensor([[ 7.7846e-03, -3.6558e-03, -4.2566e-02,  ..., -6.0273e-03,\n",
       "                        2.0292e-02, -1.0706e-03],\n",
       "                      [ 2.3298e-02, -1.4196e-02,  1.9890e-02,  ..., -3.5744e-02,\n",
       "                       -5.2866e-03, -1.5783e-03],\n",
       "                      [ 1.9778e-05, -3.7603e-02, -8.4156e-03,  ..., -2.5424e-02,\n",
       "                        8.8831e-03, -2.1328e-02],\n",
       "                      ...,\n",
       "                      [ 1.4104e-02,  4.0340e-03, -2.0924e-02,  ..., -3.8138e-02,\n",
       "                        3.0461e-03, -3.0604e-02],\n",
       "                      [ 2.4586e-02,  2.5424e-02, -6.6804e-03,  ..., -7.6707e-03,\n",
       "                       -6.4278e-03, -3.6856e-02],\n",
       "                      [ 3.6326e-02,  4.4988e-03,  3.4233e-02,  ..., -3.2292e-02,\n",
       "                       -3.0180e-02,  2.7565e-02]])),\n",
       "             ('decoder.net.attn_layers.layers.0.1.to_out.weight',\n",
       "              tensor([[ 0.0320, -0.0250,  0.0379,  ...,  0.0069,  0.0006,  0.0115],\n",
       "                      [ 0.0071,  0.0191,  0.0078,  ...,  0.0151,  0.0248,  0.0306],\n",
       "                      [-0.0369, -0.0233,  0.0056,  ...,  0.0303, -0.0302, -0.0306],\n",
       "                      ...,\n",
       "                      [-0.0028, -0.0255,  0.0260,  ..., -0.0068,  0.0411,  0.0204],\n",
       "                      [-0.0081, -0.0369,  0.0101,  ...,  0.0192, -0.0079, -0.0136],\n",
       "                      [-0.0411, -0.0422, -0.0153,  ...,  0.0207,  0.0053, -0.0424]])),\n",
       "             ('decoder.net.attn_layers.layers.0.1.to_out.bias',\n",
       "              tensor([ 2.0146e-03,  3.5073e-02, -3.8686e-02, -1.4063e-03, -3.0447e-05,\n",
       "                       3.8546e-02, -1.0426e-02,  1.6131e-02,  3.9054e-02,  4.2832e-02,\n",
       "                      -2.2987e-02, -2.5904e-02,  1.4083e-02, -2.6017e-02, -3.4745e-02,\n",
       "                       1.0879e-02,  2.5422e-02,  6.5794e-03, -2.6852e-02,  5.7401e-03,\n",
       "                      -1.6763e-02, -3.1534e-02,  8.5103e-03, -2.7710e-02, -1.0906e-02,\n",
       "                       3.0388e-02, -3.5604e-02,  3.2496e-02, -2.6160e-02,  2.4547e-02,\n",
       "                      -2.3244e-02, -3.3238e-02,  1.9510e-02,  4.1744e-02, -6.0514e-03,\n",
       "                      -2.2770e-02, -2.4805e-02,  2.6700e-02,  1.9412e-03, -1.2289e-02,\n",
       "                       7.5579e-04,  4.0951e-02, -3.6724e-02, -2.8052e-02, -1.6736e-02,\n",
       "                       1.8950e-02, -4.3970e-02, -3.2666e-02,  3.0245e-02,  1.3162e-02,\n",
       "                       3.6766e-02,  3.2535e-02,  7.9326e-03, -4.0823e-02,  4.0659e-02,\n",
       "                       3.3796e-02,  1.9108e-02,  1.2182e-02,  5.4755e-03,  2.1375e-02,\n",
       "                       1.6499e-03,  1.9115e-02, -1.1935e-02, -4.3124e-02,  9.8676e-04,\n",
       "                      -3.9679e-02, -4.1333e-02,  2.6853e-02,  8.3259e-03,  3.4337e-02,\n",
       "                      -3.0170e-03,  1.0438e-02, -1.3174e-02,  1.2653e-02, -2.0641e-02,\n",
       "                      -1.6803e-02, -1.8890e-02, -2.5754e-02, -2.9376e-02,  3.5465e-02,\n",
       "                       2.1866e-02, -5.8813e-03,  3.5333e-02,  1.9756e-02,  3.7746e-02,\n",
       "                       2.4497e-02, -3.2706e-02, -2.1357e-02, -3.7727e-02,  3.3039e-02,\n",
       "                      -3.2518e-02, -2.5067e-04, -8.7824e-03,  3.6742e-03, -3.7521e-02,\n",
       "                       3.7078e-02,  3.2736e-02, -2.0925e-02, -6.1454e-03,  2.8510e-02,\n",
       "                      -3.1782e-03,  4.1655e-02, -3.9854e-02, -3.5417e-02,  1.4091e-02,\n",
       "                       3.6317e-02, -3.8489e-02, -2.8870e-02,  3.2759e-02,  3.0610e-02,\n",
       "                       3.2882e-02,  1.5376e-02, -2.7952e-03,  3.5342e-02, -2.5326e-03,\n",
       "                       1.2713e-02,  8.7826e-03,  6.7497e-03, -3.5879e-02, -3.3735e-02,\n",
       "                       3.0711e-02, -3.7440e-02, -2.1023e-02, -3.5837e-02,  1.7521e-03,\n",
       "                      -1.2061e-02,  1.6038e-02,  3.0203e-02, -3.0346e-02,  1.9119e-02,\n",
       "                       3.3319e-02, -3.8229e-02,  2.7013e-02,  1.6574e-02,  1.0698e-02,\n",
       "                       2.3387e-02,  1.9975e-02, -2.1216e-02, -9.6481e-04,  2.1815e-02,\n",
       "                      -2.2743e-02,  3.8845e-02,  3.3153e-02,  2.5801e-03,  3.3908e-02,\n",
       "                       3.8177e-03,  4.3615e-03, -3.3214e-02,  3.4565e-02, -2.7880e-02,\n",
       "                       3.5390e-03,  4.0561e-02,  1.3824e-02,  4.3277e-02, -4.0663e-02,\n",
       "                      -2.7036e-02, -1.5718e-02,  1.2362e-02,  3.4367e-02, -1.0838e-03,\n",
       "                      -1.8927e-02, -1.2408e-02, -1.6256e-03,  3.0083e-02,  1.4110e-03,\n",
       "                       4.0609e-02, -1.2501e-02,  1.8652e-02,  4.3444e-02,  2.7167e-03,\n",
       "                       3.5677e-02, -4.1898e-02, -1.6666e-03,  2.1554e-02,  3.3392e-02,\n",
       "                       3.0568e-02, -2.3997e-02, -2.5977e-02,  5.6541e-04, -1.0439e-02,\n",
       "                      -2.4475e-02,  2.2741e-02, -9.3856e-03,  2.1947e-02,  1.4450e-02,\n",
       "                       9.8804e-03, -9.0840e-03, -3.3272e-02,  1.2143e-02,  1.4600e-02,\n",
       "                       3.1670e-02, -4.1476e-02,  3.8594e-02, -4.1068e-02, -9.1602e-03,\n",
       "                      -1.9592e-02, -2.1572e-02,  3.1473e-02, -3.6834e-03,  6.0760e-04,\n",
       "                      -5.8437e-03,  1.7686e-03, -3.4206e-02, -1.9592e-02, -2.3214e-02,\n",
       "                       4.3519e-03,  2.4052e-02,  3.1358e-02,  1.6900e-02, -1.9604e-02,\n",
       "                       1.3989e-02,  3.2028e-02, -9.3820e-03, -3.5177e-02, -3.9818e-02,\n",
       "                      -4.3893e-03, -2.0034e-03, -1.8297e-02, -7.3200e-03,  3.4330e-02,\n",
       "                       2.1091e-02, -1.4381e-02,  3.0644e-03,  3.7730e-02, -2.6556e-02,\n",
       "                       3.9127e-02, -3.7022e-02, -3.7214e-02, -2.1657e-02, -4.0245e-02,\n",
       "                      -1.5285e-02, -3.3310e-03,  2.5846e-02, -1.6497e-02,  3.8547e-02,\n",
       "                      -1.5829e-02,  2.5796e-02,  6.3169e-03, -1.5273e-02, -3.9644e-02,\n",
       "                       4.0493e-03,  4.3942e-02,  3.6905e-02, -2.1272e-02, -4.0577e-02,\n",
       "                       2.9070e-02, -1.2778e-02, -1.1272e-02, -2.2413e-03, -2.0734e-02,\n",
       "                       2.6541e-02,  1.0424e-02, -3.9328e-02,  4.0798e-02, -1.6969e-02,\n",
       "                      -4.0927e-02,  4.4937e-03,  2.2527e-02, -7.7500e-03, -2.1398e-02,\n",
       "                      -4.5503e-03, -7.2971e-03, -1.9457e-02,  4.3919e-02, -1.3140e-03,\n",
       "                       2.6555e-02, -2.2234e-02,  3.4720e-02,  3.1069e-02,  2.5207e-02,\n",
       "                       4.1755e-02,  3.6578e-02, -1.6546e-02, -3.7613e-02,  1.9589e-02,\n",
       "                      -6.3059e-04,  7.4631e-03, -5.0805e-03, -4.3075e-02,  1.2802e-02,\n",
       "                      -4.3132e-02, -1.0667e-02,  4.0872e-02, -8.6465e-03,  2.0435e-02,\n",
       "                      -2.3247e-03, -1.8087e-02,  6.2454e-04, -2.5924e-02,  3.4769e-02,\n",
       "                      -2.6997e-02,  4.2136e-02,  4.1341e-02, -2.5708e-02,  4.1236e-02,\n",
       "                       3.2829e-03, -1.9523e-02, -3.7797e-02,  7.9967e-03,  8.7696e-03,\n",
       "                       1.3745e-03, -3.3885e-02, -4.3270e-02,  9.4096e-03, -2.0644e-02,\n",
       "                       2.2548e-02, -2.1850e-02, -3.9354e-02,  1.6285e-02,  4.3722e-02,\n",
       "                       2.2108e-02, -2.5649e-03, -2.8350e-02, -3.9454e-03,  2.7659e-02,\n",
       "                      -3.3126e-03, -3.4979e-02,  4.3534e-02, -2.5497e-02,  3.6441e-03,\n",
       "                      -6.2334e-03, -1.6013e-02,  4.1897e-02,  1.6077e-02,  4.1033e-02,\n",
       "                      -1.0104e-02, -3.9482e-02, -4.2171e-02,  2.1504e-03, -1.8624e-02,\n",
       "                       1.4626e-02,  3.2534e-02,  2.2313e-02,  3.0468e-02,  3.4645e-02,\n",
       "                      -3.6303e-02,  2.6191e-02,  1.3388e-02,  1.2256e-02,  1.2517e-02,\n",
       "                      -4.0905e-02, -3.5679e-02,  2.5318e-02, -1.2350e-02,  2.4139e-02,\n",
       "                      -4.3940e-02,  3.1222e-02,  2.2226e-02,  1.3544e-02,  4.3160e-02,\n",
       "                       1.7744e-02, -3.7820e-02, -1.7526e-02, -4.4091e-02, -9.7861e-04,\n",
       "                      -4.2320e-02,  2.3799e-02, -1.6322e-02, -2.5211e-02,  1.7685e-02,\n",
       "                      -4.0127e-03,  4.2631e-02, -1.5514e-02, -4.0844e-02,  2.1363e-02,\n",
       "                       2.5767e-02,  1.8225e-02, -2.4607e-03,  3.3039e-03,  3.9818e-03,\n",
       "                      -4.2317e-02,  1.6115e-03,  3.9441e-02,  2.7829e-02, -3.0006e-02,\n",
       "                       2.9163e-02, -3.8589e-02,  2.9052e-02,  4.0580e-02,  5.2410e-03,\n",
       "                      -1.4894e-02,  3.3133e-02,  3.2965e-02,  2.2133e-02,  7.3352e-03,\n",
       "                       1.8312e-02, -1.4253e-02, -3.9931e-03,  4.2009e-02,  1.0606e-02,\n",
       "                       2.5579e-02,  4.0448e-02, -2.9185e-02,  3.5635e-02, -4.1397e-02,\n",
       "                       3.4756e-02,  3.7775e-02, -1.9440e-02, -1.5245e-02,  3.8525e-02,\n",
       "                       2.2050e-02, -2.9727e-02, -3.9988e-02,  3.6680e-02, -3.4297e-02,\n",
       "                       2.7438e-03,  1.5981e-02, -4.3772e-03, -1.9445e-02, -3.4174e-02,\n",
       "                       2.3428e-02,  3.0922e-02,  1.6337e-02, -1.1098e-02,  3.2305e-02,\n",
       "                       3.6417e-02, -2.5010e-02, -3.7184e-02, -2.3269e-03,  1.4600e-02,\n",
       "                      -4.3078e-02,  3.6301e-02, -6.8461e-03, -9.0044e-03,  1.8375e-02,\n",
       "                       2.6600e-02, -4.1157e-02,  1.3689e-02, -1.8787e-02,  1.4888e-02,\n",
       "                       1.4448e-02,  4.3677e-02,  3.4975e-02,  3.2185e-02,  3.0312e-02,\n",
       "                       1.3524e-02,  1.5804e-02, -3.6618e-02, -3.5151e-02, -3.6218e-02,\n",
       "                      -1.1091e-02,  6.9538e-03,  2.6892e-02, -3.2445e-02,  3.8938e-02,\n",
       "                      -4.3519e-02, -4.1758e-02,  2.7086e-02,  2.8873e-02,  3.7595e-02,\n",
       "                      -3.6761e-02,  5.6971e-03,  4.2655e-02,  1.3364e-02, -2.6721e-02,\n",
       "                      -4.0193e-02, -2.9744e-03, -1.1776e-02,  3.6358e-02,  4.0846e-02,\n",
       "                       1.3751e-02,  2.9667e-02, -2.1807e-02,  8.8878e-03,  2.9273e-03,\n",
       "                      -8.0740e-03, -2.5129e-02,  2.4748e-02,  1.9174e-02, -1.9970e-02,\n",
       "                       2.6716e-02, -6.9854e-03, -7.9025e-03,  2.2879e-02,  3.4071e-02,\n",
       "                      -4.6236e-03,  1.5492e-02,  7.2553e-03, -3.7779e-02,  1.1650e-02,\n",
       "                       4.3172e-02, -3.6044e-02, -3.1389e-02,  1.3444e-02, -3.1800e-02,\n",
       "                       4.2684e-02, -2.7566e-02, -1.1567e-02, -3.7280e-02,  3.1762e-02,\n",
       "                      -5.4124e-04, -2.6265e-02,  2.1776e-02, -1.2843e-02,  3.5426e-02,\n",
       "                      -3.8553e-02, -2.0141e-02,  1.2322e-02, -2.2195e-03, -1.0065e-02,\n",
       "                       2.1650e-02, -2.0536e-02,  3.1780e-02, -2.0758e-02, -2.7886e-02,\n",
       "                      -2.6060e-02,  9.8803e-03,  1.5408e-02,  7.3761e-03,  3.0132e-03,\n",
       "                      -3.2062e-02,  2.2990e-02])),\n",
       "             ('decoder.net.attn_layers.layers.1.0.0.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('decoder.net.attn_layers.layers.1.0.0.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.orig_layer.orig_layer.net.0.0.weight',\n",
       "              tensor([[ 0.0416, -0.0105,  0.0209,  ...,  0.0378,  0.0045, -0.0430],\n",
       "                      [ 0.0436, -0.0301,  0.0094,  ..., -0.0023, -0.0247,  0.0334],\n",
       "                      [ 0.0369,  0.0408, -0.0134,  ...,  0.0252,  0.0176,  0.0186],\n",
       "                      ...,\n",
       "                      [ 0.0035,  0.0366, -0.0082,  ..., -0.0334, -0.0365,  0.0091],\n",
       "                      [-0.0009, -0.0271,  0.0419,  ...,  0.0121, -0.0053,  0.0099],\n",
       "                      [-0.0364, -0.0114, -0.0245,  ...,  0.0020,  0.0373, -0.0199]])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.orig_layer.orig_layer.net.0.0.bias',\n",
       "              tensor([-0.0115,  0.0055, -0.0371,  ...,  0.0314,  0.0403,  0.0019])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.orig_layer.orig_layer.net.3.weight',\n",
       "              tensor([[-0.0096,  0.0187,  0.0179,  ..., -0.0040, -0.0009, -0.0089],\n",
       "                      [-0.0113, -0.0153, -0.0044,  ..., -0.0160, -0.0143, -0.0079],\n",
       "                      [-0.0020,  0.0192,  0.0190,  ...,  0.0185,  0.0038, -0.0124],\n",
       "                      ...,\n",
       "                      [ 0.0102, -0.0034,  0.0101,  ..., -0.0091, -0.0202,  0.0162],\n",
       "                      [ 0.0022,  0.0083, -0.0185,  ..., -0.0162, -0.0188, -0.0158],\n",
       "                      [ 0.0012,  0.0044, -0.0100,  ..., -0.0073,  0.0058, -0.0093]])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.orig_layer.orig_layer.net.3.bias',\n",
       "              tensor([ 9.4793e-03, -5.3299e-03, -1.0657e-02, -1.1068e-02,  9.5330e-03,\n",
       "                       1.7155e-02, -2.1171e-02, -1.0488e-02,  1.9682e-02,  1.0541e-02,\n",
       "                       1.3302e-02,  8.1521e-03,  1.1929e-02, -2.1981e-02,  2.1394e-02,\n",
       "                       9.2599e-03,  1.4374e-02, -1.5658e-02,  2.0548e-02, -3.7670e-03,\n",
       "                       3.5675e-03, -2.0903e-02,  4.1848e-03,  1.0696e-02,  8.3542e-03,\n",
       "                      -1.8435e-02, -1.7567e-02,  2.7016e-03,  2.4536e-03,  5.6843e-03,\n",
       "                      -1.7575e-03,  1.6371e-02, -1.8962e-02,  3.7082e-03, -1.2549e-02,\n",
       "                      -2.7539e-03,  1.4467e-02, -2.0421e-02,  2.5852e-03, -1.9824e-03,\n",
       "                      -1.1257e-02, -1.5049e-02, -1.4319e-02, -2.5248e-03, -2.1960e-02,\n",
       "                       1.1643e-02, -9.7083e-03,  3.2499e-03, -4.9143e-03, -2.1155e-02,\n",
       "                       1.0429e-02, -1.4918e-02, -1.8540e-02,  1.4960e-02, -8.6141e-03,\n",
       "                      -6.0319e-03,  1.1207e-02, -9.0385e-03,  8.4184e-03,  1.2800e-02,\n",
       "                       7.4219e-03,  1.7135e-02, -1.0549e-02, -1.5610e-02,  6.4945e-03,\n",
       "                      -1.0130e-02, -1.5267e-02,  4.0580e-03, -1.9743e-02,  1.4260e-02,\n",
       "                      -1.1056e-02, -1.2835e-02,  5.2079e-03,  4.9759e-03, -1.9011e-02,\n",
       "                       1.3871e-02,  1.4973e-02, -1.1084e-02,  2.1282e-02,  1.0993e-02,\n",
       "                      -1.5564e-02,  2.1457e-04, -1.3487e-02, -1.1862e-02, -9.1369e-03,\n",
       "                       4.7007e-03, -1.9817e-02,  4.7792e-03,  1.4853e-02,  2.0974e-02,\n",
       "                      -3.8398e-03,  1.9954e-02,  1.5352e-02, -1.1428e-02, -1.8465e-02,\n",
       "                       1.2978e-03,  1.9593e-02,  1.0416e-02,  6.3373e-03, -2.2062e-02,\n",
       "                       1.2310e-02,  2.1393e-02, -4.1253e-03, -1.5264e-02, -1.3726e-02,\n",
       "                       1.3930e-02,  1.5015e-02, -1.4149e-02,  3.9377e-03, -1.4700e-02,\n",
       "                       1.6212e-02, -1.8710e-03, -1.4121e-02, -1.7069e-02,  3.9291e-03,\n",
       "                       2.1530e-02, -2.1463e-02, -2.1935e-02, -1.4884e-02, -1.5317e-02,\n",
       "                      -2.0230e-02,  2.1698e-03, -4.7996e-03, -5.4497e-03, -1.4468e-02,\n",
       "                      -1.4733e-02, -1.9235e-02,  1.7623e-02,  5.1140e-03, -2.0850e-02,\n",
       "                      -8.6379e-03,  1.4578e-02,  1.6914e-02, -1.5837e-02, -2.9343e-03,\n",
       "                       8.0550e-03, -1.0874e-02, -3.7313e-03,  4.7540e-04, -2.2711e-03,\n",
       "                      -6.9409e-03, -3.5144e-03,  1.0258e-02, -4.7558e-03, -2.0256e-02,\n",
       "                       1.8079e-02, -1.8609e-02, -5.7263e-03, -1.3392e-02, -1.5384e-03,\n",
       "                      -8.7934e-04,  4.4574e-03,  1.7427e-03, -2.2604e-03, -1.2375e-02,\n",
       "                      -9.0586e-03,  1.6469e-02,  7.6300e-04,  1.1636e-02,  1.9320e-02,\n",
       "                       1.6000e-02,  3.5677e-04, -5.0323e-03,  5.1703e-05,  1.3613e-03,\n",
       "                       2.1023e-02, -5.0308e-03, -1.2718e-02, -1.0183e-02, -1.8693e-03,\n",
       "                      -2.1875e-02,  1.3409e-02, -7.5510e-03, -1.1719e-02, -1.8861e-02,\n",
       "                      -1.5658e-02, -3.9382e-03,  7.1095e-04,  3.4190e-03,  1.2603e-02,\n",
       "                      -1.7977e-02, -3.8107e-03, -9.2640e-03, -3.6073e-04, -7.0304e-03,\n",
       "                       2.2076e-02,  3.9594e-03, -1.1542e-02,  8.5643e-03, -2.0525e-02,\n",
       "                      -1.9459e-02, -7.5989e-03,  4.6716e-03,  1.4425e-03,  1.6314e-02,\n",
       "                      -2.0928e-02, -5.9195e-03,  1.6688e-02,  1.5455e-02, -4.8431e-03,\n",
       "                       1.9673e-02, -1.6673e-02, -1.1189e-02,  2.0332e-02,  1.9236e-02,\n",
       "                      -9.9974e-03,  6.4315e-03,  1.9975e-02, -1.4154e-03,  2.6520e-03,\n",
       "                      -2.0268e-02,  1.9945e-02, -2.1057e-02,  1.9149e-02, -1.6944e-02,\n",
       "                       1.4152e-02, -3.3318e-03, -1.7061e-02, -8.5067e-03,  2.1273e-02,\n",
       "                       1.9099e-02, -3.6389e-03,  7.8347e-03,  9.1071e-03,  1.2034e-02,\n",
       "                       1.7162e-02,  7.6494e-03, -9.2439e-03,  1.8249e-02, -1.1499e-03,\n",
       "                      -1.4179e-02, -4.5233e-03, -1.2678e-02,  1.0005e-02,  7.6625e-03,\n",
       "                      -1.5205e-03,  1.4889e-02, -1.7877e-02,  1.1973e-03,  1.9815e-02,\n",
       "                       1.8099e-02, -1.2852e-02,  2.9026e-03,  1.9310e-02,  6.3759e-04,\n",
       "                      -8.4921e-03, -4.6764e-04, -6.3185e-03, -4.0683e-03, -9.2126e-03,\n",
       "                       1.8383e-02,  1.2670e-02, -1.8196e-02,  2.1160e-03,  1.0388e-02,\n",
       "                       1.7842e-02, -3.7241e-04, -1.3362e-02,  2.1423e-02,  2.1367e-02,\n",
       "                       1.7184e-03,  1.1985e-03, -1.0752e-02,  2.1319e-03,  1.2067e-02,\n",
       "                      -4.1539e-03,  2.5168e-03,  1.5743e-03,  7.0294e-03, -8.0115e-03,\n",
       "                      -1.5969e-02,  6.7423e-03, -1.2609e-02,  6.6858e-03, -1.5777e-02,\n",
       "                      -1.2430e-02,  5.8040e-03, -8.7365e-03,  4.9809e-04,  1.7694e-02,\n",
       "                      -1.6052e-02, -1.4056e-02, -1.2284e-02,  1.2970e-02,  1.1183e-02,\n",
       "                      -1.7604e-02, -1.8512e-02,  2.0767e-02,  8.2328e-03, -1.3787e-02,\n",
       "                      -1.4208e-02,  1.7347e-02,  6.0060e-03,  1.2724e-02,  1.1260e-02,\n",
       "                       1.3271e-02,  1.8925e-02, -1.6796e-02,  1.4254e-02,  6.0041e-03,\n",
       "                       1.8086e-02,  4.6116e-03, -1.6399e-02, -7.6470e-03,  7.9213e-03,\n",
       "                      -1.2757e-02,  3.4442e-03,  1.8914e-02,  1.6945e-04,  6.5332e-03,\n",
       "                      -8.9016e-03,  2.1593e-02,  6.1644e-03, -2.1855e-02, -1.5088e-02,\n",
       "                       1.1016e-02,  3.0775e-03, -3.2320e-03, -9.5793e-03, -6.3021e-03,\n",
       "                      -2.0058e-02, -4.3393e-03, -1.3851e-02,  1.6343e-02, -1.9514e-03,\n",
       "                       2.0726e-02,  1.8900e-02, -1.9015e-02, -6.2414e-03, -8.7543e-03,\n",
       "                      -1.1042e-03, -1.1007e-02, -1.4957e-02,  1.3853e-02, -1.9066e-02,\n",
       "                       7.8149e-03,  4.2571e-03, -3.5130e-03,  2.1124e-02,  1.9969e-02,\n",
       "                       9.0493e-03,  1.0970e-02,  2.0135e-02,  1.5077e-03, -4.9598e-03,\n",
       "                       1.8685e-02, -4.2956e-05, -1.6652e-02,  1.0478e-02, -9.8768e-03,\n",
       "                      -1.2944e-02, -1.8717e-02,  1.1855e-02, -7.1189e-03,  4.9012e-03,\n",
       "                       8.4048e-03,  1.1013e-02, -3.0894e-03, -1.8070e-02, -2.2746e-03,\n",
       "                       1.9524e-02,  1.2573e-02, -1.5161e-02, -1.9870e-03,  6.2116e-03,\n",
       "                       2.1864e-02,  2.0719e-02,  1.9919e-02, -1.0623e-02, -1.5683e-02,\n",
       "                      -2.0198e-02, -1.6040e-02, -1.0754e-02, -7.5217e-03,  1.5018e-02,\n",
       "                      -4.9141e-03,  4.2914e-03, -1.4358e-02,  2.2747e-03,  5.0084e-03,\n",
       "                       1.1400e-02,  1.6409e-02, -2.1094e-03, -4.4478e-03, -9.5429e-03,\n",
       "                       7.9471e-03,  8.6445e-03, -8.8244e-04, -1.6796e-02,  1.9839e-02,\n",
       "                       6.4094e-03, -2.3014e-03, -1.8664e-02, -1.7382e-03, -2.0613e-04,\n",
       "                      -1.2580e-02,  1.0635e-02, -1.9739e-02, -1.6860e-02,  1.1931e-02,\n",
       "                      -1.7371e-02, -1.6573e-02, -1.3018e-02, -1.9091e-02,  5.4869e-03,\n",
       "                       1.1005e-03,  5.5230e-03, -1.3398e-02, -1.4708e-02, -1.8409e-02,\n",
       "                       1.7167e-02,  1.3930e-02,  3.6569e-03, -6.9480e-03,  1.8193e-03,\n",
       "                       1.1252e-03, -1.4116e-02,  1.8226e-02,  2.8706e-03,  4.2873e-03,\n",
       "                      -8.6157e-03, -3.3504e-04, -1.6787e-02, -1.7992e-02,  2.0275e-02,\n",
       "                      -5.0075e-04, -1.3669e-02,  3.4394e-03,  1.7829e-02,  1.0604e-02,\n",
       "                       9.1965e-04, -1.0540e-02,  5.9742e-03, -6.0151e-03, -6.5881e-03,\n",
       "                       2.5773e-03, -1.5041e-02,  1.3618e-02,  1.0046e-02, -7.0376e-03,\n",
       "                       1.3881e-02,  1.6845e-02,  9.8301e-03,  1.8272e-02,  1.8454e-02,\n",
       "                      -1.9820e-02, -1.1811e-02, -8.8691e-03, -1.9902e-02, -3.7607e-03,\n",
       "                      -2.1359e-02,  3.4421e-04,  1.2200e-02,  2.0255e-02,  9.5098e-03,\n",
       "                       2.0607e-02,  1.4451e-02,  8.1598e-03, -1.8469e-02, -1.2844e-02,\n",
       "                       1.5489e-02,  1.0371e-02, -1.9251e-02, -1.3847e-02, -1.7055e-02,\n",
       "                       1.5407e-02,  1.4257e-02, -4.8095e-03, -2.9343e-03,  1.0838e-02,\n",
       "                       1.1343e-02,  9.1501e-03,  2.0658e-02,  2.0856e-02,  8.2873e-04,\n",
       "                      -9.1748e-03,  1.8749e-02,  1.6816e-02, -1.8678e-02,  9.0905e-03,\n",
       "                       9.2524e-03, -5.1062e-03,  1.4497e-02,  1.4768e-02,  1.9664e-02,\n",
       "                       3.8605e-03,  1.8797e-02, -1.5307e-02,  8.4648e-03, -1.3395e-02,\n",
       "                      -1.7443e-02, -8.9463e-03,  6.2144e-03,  1.5664e-02,  2.0402e-02,\n",
       "                      -1.3171e-02,  1.9563e-02,  1.8383e-03,  1.8993e-02,  1.8763e-02,\n",
       "                       9.0094e-03, -1.5813e-02,  1.5861e-02,  9.1568e-03, -1.6763e-02,\n",
       "                       8.9690e-03,  1.4373e-02,  2.0733e-02,  1.2935e-02, -1.7833e-02,\n",
       "                       7.2389e-03,  1.5944e-02])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.orig_layer.adapter.adapter_block.0.weight',\n",
       "              tensor([[0.8649]])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.orig_layer.adapter.adapter_block.0.bias',\n",
       "              tensor([-0.4102])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.orig_layer.adapter.adapter_block.2.weight',\n",
       "              tensor([[-0.7662]])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.orig_layer.adapter.adapter_block.2.bias',\n",
       "              tensor([-0.1895])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.adapter.adapter_block.0.weight',\n",
       "              tensor([[0.8688]])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.adapter.adapter_block.0.bias',\n",
       "              tensor([0.3200])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.adapter.adapter_block.2.weight',\n",
       "              tensor([[-0.4071]])),\n",
       "             ('decoder.net.attn_layers.layers.1.1.adapter.adapter_block.2.bias',\n",
       "              tensor([0.2296])),\n",
       "             ('decoder.net.norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('decoder.net.norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('decoder.net.to_logits.0.weight',\n",
       "              tensor([[ 0.0179, -0.0240, -0.0313,  ...,  0.0223, -0.0095,  0.0040],\n",
       "                      [ 0.0362, -0.0070, -0.0250,  ..., -0.0181, -0.0177,  0.0370],\n",
       "                      [-0.0229,  0.0235,  0.0382,  ..., -0.0287, -0.0192,  0.0309],\n",
       "                      [-0.0262,  0.0358, -0.0131,  ..., -0.0040, -0.0036,  0.0436],\n",
       "                      [-0.0058, -0.0397,  0.0409,  ..., -0.0112,  0.0051, -0.0067]])),\n",
       "             ('decoder.net.to_logits.0.bias',\n",
       "              tensor([ 0.0441,  0.0395,  0.0123, -0.0226,  0.0057])),\n",
       "             ('decoder.net.to_logits.1.weight',\n",
       "              tensor([[-1.9498e-02, -1.8580e-02,  2.3966e-02,  ..., -3.7127e-02,\n",
       "                       -3.0961e-02,  3.5359e-02],\n",
       "                      [-2.4574e-03, -4.1646e-02, -2.6129e-02,  ...,  1.1003e-02,\n",
       "                       -1.9889e-02,  1.9729e-04],\n",
       "                      [ 2.6721e-03,  1.6697e-02,  2.9018e-02,  ...,  2.4911e-03,\n",
       "                       -2.6586e-02, -6.0387e-06],\n",
       "                      ...,\n",
       "                      [ 3.9751e-02,  3.4174e-02, -3.5925e-03,  ...,  4.0957e-02,\n",
       "                       -3.9233e-02, -2.6668e-02],\n",
       "                      [ 1.1322e-02, -1.2068e-02, -1.1794e-02,  ..., -2.9117e-02,\n",
       "                        1.1710e-02, -3.7846e-02],\n",
       "                      [ 1.8816e-02, -2.6298e-02,  1.5156e-02,  ...,  3.5010e-02,\n",
       "                       -1.2624e-02, -1.0123e-02]])),\n",
       "             ('decoder.net.to_logits.1.bias',\n",
       "              tensor([-1.8758e-02,  9.7906e-03, -1.2917e-02,  1.9797e-02,  2.0107e-02,\n",
       "                      -1.6237e-02,  1.2055e-02, -3.8111e-02,  1.0919e-02,  4.3208e-02,\n",
       "                      -2.7271e-02, -2.3011e-02, -1.7583e-02, -3.7441e-03, -3.5027e-02,\n",
       "                      -4.2889e-02,  6.6385e-04, -7.3741e-03, -2.5692e-02,  3.9794e-02,\n",
       "                      -4.1191e-02,  2.4376e-02,  5.6561e-03, -2.7461e-02, -4.3461e-02,\n",
       "                      -2.1613e-02, -1.3588e-02,  3.0897e-02,  3.6938e-02, -1.0998e-03,\n",
       "                      -9.0856e-03,  3.2515e-02,  1.1457e-02,  1.6944e-02, -2.6673e-02,\n",
       "                      -2.7137e-02, -5.8195e-03, -3.6418e-02,  3.9769e-02, -3.8231e-02,\n",
       "                       3.8577e-02,  9.9214e-03,  2.7484e-04,  1.8955e-02,  2.5828e-02,\n",
       "                      -1.0801e-02, -3.4019e-02,  3.8173e-02, -1.1568e-02,  1.1669e-02,\n",
       "                       1.9557e-02,  3.0797e-02, -3.6896e-02, -2.8012e-02, -2.2420e-02,\n",
       "                       6.9265e-03, -1.3313e-02, -4.3426e-02,  2.7409e-02,  1.9924e-02,\n",
       "                       1.7807e-02, -1.7777e-02, -1.1466e-02, -1.3656e-02, -3.2924e-02,\n",
       "                       2.1003e-02, -1.8496e-02, -3.6358e-02,  9.0732e-04,  7.0716e-04,\n",
       "                       2.7720e-02, -4.4145e-02, -3.6934e-02, -2.8765e-02,  4.1652e-02,\n",
       "                      -2.4842e-02, -3.9225e-02,  2.9640e-02, -1.4375e-03,  1.0263e-02,\n",
       "                       1.1393e-02, -2.5327e-02, -3.3294e-02,  3.1165e-02,  3.2085e-02,\n",
       "                      -3.9318e-02,  1.9742e-02,  3.8233e-02, -4.0758e-03,  4.1926e-03,\n",
       "                      -6.0553e-03, -2.8669e-02, -2.8992e-04,  2.4370e-02,  5.4958e-03,\n",
       "                      -1.5699e-02,  2.9082e-02,  4.3969e-02,  1.7211e-02,  3.7298e-02,\n",
       "                       2.4420e-02, -3.3971e-02,  1.4430e-02,  3.4439e-02,  6.3653e-03,\n",
       "                      -2.1417e-03, -4.0896e-02,  4.1329e-02, -4.3685e-02,  4.1275e-02,\n",
       "                      -1.6320e-02, -3.4565e-03,  2.6853e-02, -2.2161e-02,  3.0060e-02,\n",
       "                       6.3827e-03, -2.0755e-02,  5.5724e-03, -4.3792e-02,  4.4396e-03,\n",
       "                       2.7631e-02,  2.3480e-02,  3.7769e-02, -4.2762e-02,  7.4303e-04,\n",
       "                      -3.6746e-02,  3.0737e-02, -2.1770e-02,  2.9277e-02, -2.4786e-02,\n",
       "                      -2.1114e-02, -3.4019e-02, -1.5031e-02,  3.6613e-02, -1.6533e-02,\n",
       "                       8.6016e-03,  1.0475e-02, -2.3221e-02,  7.6326e-03, -1.4602e-02,\n",
       "                      -1.5536e-02, -5.8282e-03, -1.5973e-02, -7.7646e-05, -3.3531e-02,\n",
       "                      -4.0848e-02, -3.6009e-02,  2.1741e-02,  5.5420e-03, -4.0243e-02,\n",
       "                       9.1974e-03,  2.5748e-02,  2.3625e-02,  1.5033e-02, -1.5015e-02,\n",
       "                      -5.0902e-03,  4.2227e-02, -1.1479e-02, -2.9225e-02,  3.9443e-02,\n",
       "                       3.6423e-02,  1.8651e-02,  6.5686e-03, -6.9118e-03,  8.6284e-03,\n",
       "                      -2.9554e-02,  3.7625e-02,  1.5433e-02,  3.8397e-02,  3.3701e-02,\n",
       "                      -6.9564e-03, -1.8336e-02, -3.7058e-03,  3.9046e-03,  2.7288e-02,\n",
       "                      -1.4264e-02,  6.2109e-03,  6.6450e-03,  2.8998e-02, -2.8455e-02,\n",
       "                       2.9297e-02,  3.4229e-02,  2.8703e-02,  9.7809e-03,  9.3159e-03,\n",
       "                      -9.3002e-03, -2.6987e-02, -3.5624e-02, -2.8239e-02, -1.9329e-02,\n",
       "                       2.3851e-02, -1.3296e-02, -1.6010e-02,  2.1312e-02,  8.2146e-03,\n",
       "                       1.3452e-02, -5.5500e-03,  1.1726e-02,  2.7498e-02, -1.7427e-02,\n",
       "                       4.0909e-02, -2.5815e-02,  2.2218e-02, -4.3781e-02, -1.9455e-02,\n",
       "                      -3.7440e-02,  2.1310e-03,  2.7102e-02,  4.7391e-04, -5.4240e-04,\n",
       "                      -1.6085e-02, -7.5136e-03, -2.2537e-02,  4.3962e-02, -8.2033e-04,\n",
       "                      -2.2992e-02, -3.1992e-02,  3.9706e-02, -2.3366e-02, -3.0772e-02,\n",
       "                       3.3465e-02, -8.3611e-03, -1.1249e-02,  2.0096e-02, -1.9003e-02,\n",
       "                      -2.2213e-02, -4.8898e-03, -2.6155e-03,  3.7956e-02,  3.7637e-02,\n",
       "                      -2.4938e-02,  3.5608e-02, -1.6309e-02,  3.5585e-02, -8.0358e-03,\n",
       "                       3.8446e-02, -2.1256e-02, -5.6564e-03,  4.3457e-02,  8.6280e-03,\n",
       "                      -1.4427e-02, -2.1650e-02, -3.7104e-02,  3.6007e-02,  2.9769e-02,\n",
       "                      -2.0281e-02,  1.9076e-02,  7.3290e-03, -2.4827e-02, -2.9729e-02,\n",
       "                       4.0453e-02, -2.2542e-02,  2.0022e-02, -2.1580e-02,  4.3975e-02,\n",
       "                       2.8615e-02, -3.9783e-02])),\n",
       "             ('decoder.net.to_logits.2.weight',\n",
       "              tensor([[-0.0289, -0.0397, -0.0273,  ..., -0.0221,  0.0372,  0.0135],\n",
       "                      [ 0.0049, -0.0422, -0.0320,  ...,  0.0094, -0.0351,  0.0390],\n",
       "                      [-0.0323, -0.0215,  0.0074,  ..., -0.0113,  0.0069, -0.0366],\n",
       "                      ...,\n",
       "                      [-0.0137, -0.0264,  0.0092,  ..., -0.0194,  0.0426,  0.0340],\n",
       "                      [ 0.0395, -0.0264, -0.0319,  ..., -0.0325,  0.0013, -0.0144],\n",
       "                      [ 0.0313, -0.0357,  0.0411,  ...,  0.0363, -0.0390,  0.0422]])),\n",
       "             ('decoder.net.to_logits.2.bias',\n",
       "              tensor([ 0.0290,  0.0085, -0.0149, -0.0095,  0.0159,  0.0312,  0.0142, -0.0050,\n",
       "                      -0.0117, -0.0172, -0.0255, -0.0337,  0.0036])),\n",
       "             ('decoder.net.to_logits.3.weight',\n",
       "              tensor([[-0.0276,  0.0051,  0.0400,  ...,  0.0119,  0.0350,  0.0348],\n",
       "                      [-0.0316, -0.0183,  0.0014,  ..., -0.0025,  0.0067,  0.0436],\n",
       "                      [ 0.0132,  0.0213, -0.0162,  ...,  0.0080, -0.0379, -0.0348],\n",
       "                      ...,\n",
       "                      [-0.0317,  0.0003,  0.0171,  ..., -0.0353,  0.0348,  0.0312],\n",
       "                      [-0.0253,  0.0186, -0.0263,  ..., -0.0064, -0.0440, -0.0338],\n",
       "                      [-0.0192, -0.0189, -0.0134,  ..., -0.0258, -0.0200,  0.0031]])),\n",
       "             ('decoder.net.to_logits.3.bias',\n",
       "              tensor([-0.0265, -0.0026,  0.0262,  0.0373,  0.0401,  0.0373,  0.0403,  0.0042,\n",
       "                       0.0292,  0.0286, -0.0195,  0.0389, -0.0057,  0.0295,  0.0038, -0.0004,\n",
       "                      -0.0305,  0.0082,  0.0321,  0.0369, -0.0404,  0.0371,  0.0114, -0.0411,\n",
       "                      -0.0288, -0.0335,  0.0074,  0.0355, -0.0278,  0.0031,  0.0401, -0.0160,\n",
       "                      -0.0096,  0.0212,  0.0306, -0.0022, -0.0165,  0.0232, -0.0422,  0.0137,\n",
       "                       0.0274,  0.0404,  0.0262, -0.0118, -0.0003, -0.0299,  0.0263, -0.0337,\n",
       "                       0.0187,  0.0318,  0.0310,  0.0358, -0.0229,  0.0273,  0.0122,  0.0214,\n",
       "                      -0.0355, -0.0062, -0.0334, -0.0048, -0.0059, -0.0089, -0.0306,  0.0093,\n",
       "                       0.0041, -0.0289, -0.0116, -0.0023, -0.0102, -0.0010, -0.0388,  0.0391,\n",
       "                       0.0096,  0.0188,  0.0323, -0.0394, -0.0162,  0.0007,  0.0103,  0.0335,\n",
       "                      -0.0059,  0.0376,  0.0271,  0.0349,  0.0398, -0.0344,  0.0345, -0.0366,\n",
       "                      -0.0012, -0.0135,  0.0084, -0.0158,  0.0220, -0.0316,  0.0385,  0.0109,\n",
       "                      -0.0382, -0.0205, -0.0310, -0.0205,  0.0101, -0.0377,  0.0352,  0.0424,\n",
       "                      -0.0358,  0.0203, -0.0212,  0.0426, -0.0362, -0.0009,  0.0429,  0.0270,\n",
       "                       0.0398,  0.0048, -0.0174, -0.0165,  0.0073,  0.0375, -0.0233, -0.0108,\n",
       "                       0.0323,  0.0051,  0.0133, -0.0393,  0.0226, -0.0324, -0.0240, -0.0437,\n",
       "                      -0.0355])),\n",
       "             ('decoder.net.to_logits.4.weight',\n",
       "              tensor([[-0.0011,  0.0365, -0.0153,  ...,  0.0440,  0.0109, -0.0312],\n",
       "                      [ 0.0200, -0.0312,  0.0222,  ...,  0.0056, -0.0126, -0.0063],\n",
       "                      [ 0.0056,  0.0099, -0.0280,  ...,  0.0144, -0.0109, -0.0010],\n",
       "                      ...,\n",
       "                      [-0.0226,  0.0219,  0.0066,  ..., -0.0225,  0.0100, -0.0174],\n",
       "                      [-0.0050,  0.0076,  0.0425,  ..., -0.0163,  0.0342,  0.0193],\n",
       "                      [ 0.0018,  0.0257,  0.0023,  ...,  0.0379, -0.0125, -0.0242]])),\n",
       "             ('decoder.net.to_logits.4.bias',\n",
       "              tensor([-0.0068, -0.0298, -0.0343, -0.0098,  0.0124, -0.0009, -0.0132,  0.0192,\n",
       "                       0.0127,  0.0077, -0.0223,  0.0366, -0.0214,  0.0321,  0.0187,  0.0315,\n",
       "                      -0.0315, -0.0240,  0.0241, -0.0006, -0.0253,  0.0101,  0.0172,  0.0314,\n",
       "                       0.0334,  0.0362, -0.0196, -0.0439,  0.0213,  0.0080,  0.0003, -0.0104,\n",
       "                      -0.0063])),\n",
       "             ('decoder.net.to_logits.5.weight',\n",
       "              tensor([[ 0.0356, -0.0018,  0.0371,  ...,  0.0328,  0.0232, -0.0135],\n",
       "                      [ 0.0299,  0.0199,  0.0151,  ..., -0.0297,  0.0005,  0.0223],\n",
       "                      [ 0.0178,  0.0350,  0.0231,  ...,  0.0435,  0.0020,  0.0114],\n",
       "                      ...,\n",
       "                      [-0.0340, -0.0065,  0.0364,  ...,  0.0321, -0.0331, -0.0312],\n",
       "                      [ 0.0147,  0.0103, -0.0385,  ...,  0.0364, -0.0383, -0.0020],\n",
       "                      [ 0.0154, -0.0365,  0.0011,  ...,  0.0024, -0.0247, -0.0221]])),\n",
       "             ('decoder.net.to_logits.5.bias',\n",
       "              tensor([ 0.0354, -0.0108,  0.0424, -0.0127, -0.0243, -0.0221,  0.0089,  0.0018,\n",
       "                      -0.0279, -0.0215,  0.0355, -0.0245,  0.0428, -0.0327,  0.0247, -0.0364,\n",
       "                       0.0287,  0.0016, -0.0189,  0.0201, -0.0313, -0.0159,  0.0005, -0.0128,\n",
       "                       0.0422, -0.0033, -0.0337, -0.0226, -0.0280,  0.0194,  0.0037,  0.0256,\n",
       "                       0.0364, -0.0299,  0.0411,  0.0209,  0.0416,  0.0048,  0.0021, -0.0342,\n",
       "                      -0.0280,  0.0235,  0.0354,  0.0375, -0.0220,  0.0031, -0.0029,  0.0213,\n",
       "                      -0.0232,  0.0287,  0.0360,  0.0091, -0.0316,  0.0087, -0.0334, -0.0233,\n",
       "                      -0.0356,  0.0307, -0.0307,  0.0309, -0.0394,  0.0250, -0.0318, -0.0108,\n",
       "                      -0.0272]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Adapters\n",
    "# Adapter model\n",
    "#smodel=copy.deepcopy(model)\n",
    "for i in range (0,len(smodel.decoder.net.attn_layers.layers)):\n",
    "    smodel.decoder.net.attn_layers.layers[1][1]=Adaptered(smodel.decoder.net.attn_layers.layers[1][1])\n",
    "smodel.state_dict()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating based on ['anglebert_fugue_3_(c)mccoy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                      | 1/2 [01:09<01:09, 69.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating based on ['albinoni_sonate_da_chiesa_6_(c)icking-archive']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [01:35<00:00, 47.84s/it]\n"
     ]
    }
   ],
   "source": [
    "advUtils.generate(2,\"./samples/TransferLearned\",model,test_loader,encoding,\"cpu\",seq_len=50,modes=[\"4_beat\",\"16_beat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:09<00:00,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating based on ['anglebert_fugue_3_(c)mccoy']\n",
      "Generating based on ['albinoni_sonate_da_chiesa_6_(c)icking-archive']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "advUtils.generate(2,\"./samples2\",smodel,test_loader,encoding,\"cpu\",seq_len=50,modes=[\"unconditioned\",\"instrument_informed\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
